import pandas as pd

df1=pd.read_excel('./data/2/è®­ç»ƒé›†/å…¥åº“æµé‡æ•°æ®.xlsx')
print(df1.head())
print(df1.shape)
print(df1.info())
print('#####################################################################')
df2=pd.read_excel('./data/2/è®­ç»ƒé›†/ç¯å¢ƒè§‚æµ‹æ•°æ®.xlsx')
print(df2.head())
print(df2.shape)
print(df2.info())
print('#####################################################################')
df3=pd.read_excel('./data/2/è®­ç»ƒé›†/é¥æµ‹ç«™é™é›¨æ•°æ®.xlsx')
print(df3.head())
print(df3.shape)
print(df3.info())
print('#####################################################################')
df4=pd.read_excel('./data/2/è®­ç»ƒé›†/é™é›¨é¢„æŠ¥æ•°æ®.xlsx')
print(df4.head())
print(df4.shape)
print(df4.info())
print('#####################################################################')
df5=pd.read_csv('./data/2/æäº¤æ¨¡æ¿/åˆèµ›_æäº¤æ¨¡æ¿.csv')
print(df5.head())
print(df5.shape)
print(df1.head())
df1['TimeStample'] = pd.to_datetime(df1['TimeStample'])
df1 = df1.set_index('TimeStample')

# ç”Ÿæˆå®Œæ•´æ—¶é—´åºåˆ—ï¼ˆ3å°æ—¶é¢‘ç‡ï¼‰
full_range = pd.date_range(start='2017-04-01 00:00:00',
                           end='2021-12-31 23:59:59',
                           freq='3H')

# é‡æ–°ç´¢å¼• â†’ æ£€æŸ¥ç¼ºå¤±
df_full = df1.reindex(full_range)

# ç¼ºå¤±æ•°é‡
print("ç¼ºå¤±ç‚¹æ•°:", df_full['Qi'].isna().sum())
# df1['TimeStample'] = pd.to_datetime(df1['TimeStample'])
# df1 = df1.sort_values('TimeStample').set_index('TimeStample')

# è®¡ç®—ç›¸é‚»æ—¶é—´é—´éš”
diffs = df1.index.to_series().diff()

# ä¸æ˜¯3å°æ—¶çš„ç‚¹
bad_points = df1.index[diffs != pd.Timedelta(hours=3)]
print("ä¸è¿ç»­ç‚¹æ•°é‡:", len(bad_points))
print("ç¤ºä¾‹:", bad_points[:10])
print(df1.shape)
print(df1.head())
print(13555/3)
print((13555+2)/3)
df6=pd.read_excel('./data/2/æµ‹è¯•é›†_åˆèµ›/é¢„æµ‹01/å…¥åº“æµé‡æ•°æ®.xlsx')
print(df6.shape)
df6=pd.read_excel('./data/2/æµ‹è¯•é›†_åˆèµ›/é¢„æµ‹02/å…¥åº“æµé‡æ•°æ®.xlsx')
print(df6.shape)
df6=pd.read_excel('./data/2/æµ‹è¯•é›†_åˆèµ›/é¢„æµ‹03/å…¥åº“æµé‡æ•°æ®.xlsx')
print(df6.shape)
# ç”Ÿæˆå®Œæ•´æ—¶é—´ç´¢å¼•ï¼ˆåªæŒ‰ç°æœ‰æ•°æ®çš„é¦–å°¾æ¥ï¼‰
full_range = pd.date_range(start=df1.index.min(),
                           end=df1.index.max(),
                           freq='3H')

# æŒ‰å®Œæ•´ç´¢å¼•é‡å»º
df_full = df1.reindex(full_range)

print("è¡¥é½åçš„ç¼ºå¤±ç‚¹æ•°:", df_full['Qi'].isna().sum())
print("ç¼ºå¤±ç‚¹ä½ç½®:\n", df_full[df_full['Qi'].isna()].head())

# æ’å€¼è¡¥é½
df_full['Qi'] = df_full['Qi'].interpolate(method='time')

# æ£€æŸ¥æ˜¯å¦è¡¥ä¸Š
print("è¡¥é½åç¼ºå¤±ç‚¹æ•°:", df_full['Qi'].isna().sum())
# æ‰¾å‡ºæ‰€æœ‰ç¼ºå¤±ç‚¹
missing_points = df1[df1['Qi'].isna()].index

# åˆ†æ®µæŸ¥çœ‹ç¼ºå¤±åŒºé—´
from itertools import groupby
from operator import itemgetter

# è¿ç»­ç¼ºå¤±ç‚¹åˆ†ç»„
groups = []
for k, g in groupby(enumerate(missing_points), lambda ix: ix[0] - ix[1].value):
    group = list(map(itemgetter(1), g))
    groups.append((group[0], group[-1], len(group)))

for start, end, count in groups:
    print(f"ç¼ºå¤±åŒºé—´: {start} â†’ {end}, å…± {count} ç‚¹")
print(df_full.shape)
print(13888/3)
# å‡è®¾ä½ å·²æœ‰ df_full (3H index), df3 (hourly), df2 (daily env)
len_3h = df_full.shape[0]
len_hourly = df3.shape[0]
len_daily = df2.shape[0]

print("3-hour points:", len_3h)
print("hourly points:", len_hourly)
print("daily points:", len_daily)
print("hourly / 3 == 3-hour? ->", len_hourly / 3, len_3h)
print("3-hour / 8 == days? ->", len_3h / 8, len_daily)
import matplotlib.pyplot as plt

# åªçœ‹ 2019 å¹´çš„æ•°æ®
subset = df_full['2019-01-01':'2019-12-31']

plt.figure(figsize=(15,5))
plt.plot(subset.index, subset['Qi'], label='2019 Inflow', color='orange')

plt.title("Reservoir Inflow in 2019")
plt.xlabel("Time")
plt.ylabel("Qi")
plt.legend()
plt.grid(True)
plt.show()
df_full['Qi_smooth'] = df_full['Qi'].rolling(window=24, min_periods=1).mean()  # ç›¸å½“äº3å¤©çš„çª—å£

plt.figure(figsize=(15,5))
plt.plot(df_full.index, df_full['Qi'], alpha=0.3, label='Raw Qi')
plt.plot(df_full.index, df_full['Qi_smooth'], color='red', label='Smoothed Qi (3-day rolling mean)')

plt.title("Reservoir Inflow (Smoothed)")
plt.xlabel("Time")
plt.ylabel("Qi")
plt.legend()
plt.grid(True)
plt.show()

print(df_full.head())
print(df_full.shape)
df61=pd.read_excel('./data/2/æµ‹è¯•é›†_å¤èµ›/é¢„æµ‹01/å…¥åº“æµé‡æ•°æ®.xlsx')
print(df61.shape)
df62=pd.read_excel('./data/2/æµ‹è¯•é›†_å¤èµ›/é¢„æµ‹02/å…¥åº“æµé‡æ•°æ®.xlsx')
print(df62.shape)
df63=pd.read_excel('./data/2/æµ‹è¯•é›†_å¤èµ›/é¢„æµ‹03/å…¥åº“æµé‡æ•°æ®.xlsx')
print(df63.shape)
print(df1.head())
from sklearn.preprocessing import StandardScaler

dfs_test = [df61, df62, df63]

# åªå– Qi åˆ—ï¼Œä¿è¯åˆ—åä¸€è‡´
test_qi_list = [d[['Qi']] for d in dfs_test]

# æ‹¼æ¥æˆä¸€ä¸ªå¤§ DataFrame
concat_all = pd.concat([df_full[['Qi']]] + test_qi_list, axis=0)
print("æ‹¼æ¥åçš„ shape:", concat_all.shape)

scaler = StandardScaler()
scaler.fit(concat_all)  # è®¡ç®—å…¨å±€å‡å€¼å’Œæ ‡å‡†å·®

print("å…¨å±€å‡å€¼:", scaler.mean_[0])
print("å…¨å±€æ ‡å‡†å·®:", scaler.scale_[0])

# å¯¹è®­ç»ƒé›†æ ‡å‡†åŒ–
df_full['Qi_norm'] = scaler.transform(df_full[['Qi']])

# å¯¹æµ‹è¯•é›†æ ‡å‡†åŒ–
for i, d in enumerate(dfs_test):
    d['Qi_norm'] = scaler.transform(d[['Qi']])

print(df_full.head())

import torch
from torch.utils.data import Dataset, DataLoader, random_split
import numpy as np

class SlidingWindowDataset(Dataset):
    # ç§»é™¤ mean, std, series_norm, ä»¥åŠå†…éƒ¨çš„æ ‡å‡†åŒ–ä»£ç 
    def __init__(self, series, hist_len=240, pred_len=56, stride=1): # ç§»é™¤ mean, std å‚æ•°
        # series å·²ç»æ˜¯å¤–éƒ¨æ ‡å‡†åŒ–åçš„ NumPy æ•°ç»„
        self.series = series.astype(np.float32)
        self.hist_len = hist_len
        self.pred_len = pred_len
        self.stride = stride

        # ----------------------------------------------------
        # âš ï¸ å…³é”®ï¼šç§»é™¤å†…éƒ¨æ ‡å‡†åŒ–ä»£ç 
        # self.mean = np.mean(self.series) if mean is None else mean
        # self.std = np.std(self.series) if std is None else std
        # self.series_norm = (self.series - self.mean) / self.std
        self.series_norm = self.series # ç›´æ¥ä½¿ç”¨ä¼ å…¥çš„æ ‡å‡†åŒ–æ•°æ®
        # ----------------------------------------------------

        self.samples = []
        for start in range(0, len(self.series_norm) - hist_len - pred_len + 1, stride):
            # hist = self.series_norm[start:start+hist_len]
            # futr = self.series_norm[start+hist_len:start+hist_len+pred_len]
            hist = self.series_norm.iloc[start:start+hist_len].values
            futr = self.series_norm.iloc[start+hist_len:start+hist_len+pred_len].values
            self.samples.append((hist, futr))

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        hist, futr = self.samples[idx]
        # ----------------------------------------------------
        # âš ï¸ å…³é”®ï¼šç§»é™¤ __getitem__ ä¸­çš„ unsqueeze(-1)
        # æˆ‘ä»¬å°†åœ¨ DataLoader å¤–éƒ¨å¤„ç†ç»´åº¦ [B, L] -> [B, L, 1]
        return (
            torch.tensor(hist),  # [hist_len]
            torch.tensor(futr)   # [pred_len]
        )
        # ----------------------------------------------------

import torch
import torch.nn as nn

class moving_avg(nn.Module):
    """
    Moving average block to highlight the trend of time series
    """
    def __init__(self, kernel_size, stride):
        super(moving_avg, self).__init__()
        self.kernel_size = kernel_size
        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)

    def forward(self, x):
        # padding on the both ends of time series
        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)
        x = torch.cat([front, x, end], dim=1)
        x = self.avg(x.permute(0, 2, 1))
        x = x.permute(0, 2, 1)

        return x


class series_decomp(nn.Module):
    """
    Series decomposition block
    """
    def __init__(self, kernel_size):
        super(series_decomp, self).__init__()
        self.moving_avg = moving_avg(kernel_size, stride=1)

    def forward(self, x):
        moving_mean = self.moving_avg(x)
        res = x - moving_mean
        return res, moving_mean

class Model(nn.Module):
    """
    Decomposition-Linear
    """
    def __init__(self, configs):
        super(Model, self).__init__()
        self.seq_len = configs.seq_len
        self.pred_len = configs.pred_len

        # Decompsition Kernel Size
        kernel_size = 25
        self.decompsition = series_decomp(kernel_size)
        self.individual = configs.individual
        self.channels = configs.enc_in

        if self.individual:
            self.Linear_Seasonal = nn.ModuleList()
            self.Linear_Trend = nn.ModuleList()

            for i in range(self.channels):
                self.Linear_Seasonal.append(nn.Linear(self.seq_len,self.pred_len))
                self.Linear_Trend.append(nn.Linear(self.seq_len,self.pred_len))

                # Use this two lines if you want to visualize the weights
                # self.Linear_Seasonal[i].weight = nn.Parameter((1/self.seq_len)*torch.ones([self.pred_len,self.seq_len]))
                # self.Linear_Trend[i].weight = nn.Parameter((1/self.seq_len)*torch.ones([self.pred_len,self.seq_len]))
        else:
            self.Linear_Seasonal = nn.Linear(self.seq_len,self.pred_len)
            self.Linear_Trend = nn.Linear(self.seq_len,self.pred_len)

            # Use this two lines if you want to visualize the weights
            # self.Linear_Seasonal.weight = nn.Parameter((1/self.seq_len)*torch.ones([self.pred_len,self.seq_len]))
            # self.Linear_Trend.weight = nn.Parameter((1/self.seq_len)*torch.ones([self.pred_len,self.seq_len]))

    def forward(self, x):
        # x: [Batch, Input length, Channel]
        seasonal_init, trend_init = self.decompsition(x)
        seasonal_init, trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1)
        if self.individual:
            seasonal_output = torch.zeros([seasonal_init.size(0),seasonal_init.size(1),self.pred_len],dtype=seasonal_init.dtype).to(seasonal_init.device)
            trend_output = torch.zeros([trend_init.size(0),trend_init.size(1),self.pred_len],dtype=trend_init.dtype).to(trend_init.device)
            for i in range(self.channels):
                seasonal_output[:,i,:] = self.Linear_Seasonal[i](seasonal_init[:,i,:])
                trend_output[:,i,:] = self.Linear_Trend[i](trend_init[:,i,:])
        else:
            seasonal_output = self.Linear_Seasonal(seasonal_init)
            trend_output = self.Linear_Trend(trend_init)

        x = seasonal_output + trend_output
        return x.permute(0,2,1) # to [Batch, Output length, Channel]

def weighted_nse(y_true, y_pred, w1=0.65, w2=0.35):
    y_true, y_pred = np.array(y_true), np.array(y_pred)
    num1 = np.sum((y_true[:16] - y_pred[:16])**2)
    denom1 = np.sum((y_true[:16] - np.mean(y_true[:16]))**2) + 1e-6
    num2 = np.sum((y_true[16:] - y_pred[16:])**2)
    denom2 = np.sum((y_true[16:] - np.mean(y_true[16:]))**2) + 1e-6
    nse = 1 - w1 * (num1/denom1) - w2 * (num2/denom2)
    return nse

def evaluate(model, loader, scaler, device="cuda"):
    model.eval()
    preds, trues = [], []
    with torch.no_grad():
        for x, y in loader:
            x, y = x.to(device), y.to(device)
            x = x.unsqueeze(-1)
            #print(x.shape)
            y = y.unsqueeze(-1)
            out = model(x)
            out = scaler.inverse_transform(out.squeeze(-1).cpu().numpy())
            y = scaler.inverse_transform(y.squeeze(-1).cpu().numpy())
            preds.append(out)
            trues.append(y)
    preds = np.concatenate(preds, axis=0)
    trues = np.concatenate(trues, axis=0)

    nse_scores = [weighted_nse(trues[i], preds[i]) for i in range(len(preds))]
    nse = np.mean(nse_scores)
    mae = np.mean(np.abs(preds - trues))
    rmse = np.sqrt(np.mean((preds - trues) ** 2))
    return nse, mae, rmse

# =====================
# 4. è®­ç»ƒå‡½æ•°
# =====================
def train_model(df_full, scaler, hist_len=240, pred_len=56, epochs=100, batch_size=32, lr=1e-4, device="cuda"):
    dataset = SlidingWindowDataset(df_full["Qi_norm"], hist_len, pred_len)
    train_size = int(len(dataset)*0.8)
    val_size = len(dataset) - train_size
    train_ds, val_ds = random_split(dataset, [train_size, val_size])
    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)

    configs = type("cfg", (), {})()
    configs.seq_len = hist_len
    configs.pred_len = pred_len
    configs.enc_in = 1
    configs.individual = False
    model = Model(configs).to(device)

    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    criterion = nn.MSELoss()   # ğŸš©è®­ç»ƒæ—¶ç”¨MSE

    for epoch in range(epochs):
        model.train()
        losses = []
        for x, y in train_loader:
            x, y = x.to(device), y.to(device)
            #print(x.shape)
            x = x.unsqueeze(-1)
            #print(x.shape)
            y = y.unsqueeze(-1)
            out = model(x)
            loss = criterion(out.squeeze(-1), y.squeeze(-1))  # MSE
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            losses.append(loss.item())

        # ===== éªŒè¯ç”¨ NSE =====
        nse, mae, rmse = evaluate(model, val_loader, scaler, device)
        print(f"Epoch {epoch+1}/{epochs} | Train MSE: {np.mean(losses):.6f} "
              f"| Val NSE: {nse:.4f} | Val MAE: {mae:.4f} | Val RMSE: {rmse:.4f}")

    return model

device = "cuda" if torch.cuda.is_available() else "cpu"
print(df_full.head())
model = train_model(df_full, scaler, hist_len=240, pred_len=56, epochs=100, device=device)

print(df61.head())
# ==========================
# é¢„æµ‹å¹¶é€†å½’ä¸€åŒ–
# ==========================
model.eval()
preds = []

for df in [df61['Qi_norm'], df62['Qi_norm'], df63['Qi_norm']]:
    # å‡†å¤‡è¾“å…¥
    #x = prepare_input(df, scaler, hist_len=240)  # [hist_len]
    hist_len=240
    x = torch.tensor(df[-hist_len:], dtype=torch.float32)  # åªå–æœ€å hist_len
    x = torch.tensor(x, dtype=torch.float32).unsqueeze(0).unsqueeze(-1).to(device)  # [1, hist_len, 1]

    with torch.no_grad():
        out = model(x)  # [1, pred_len, 1]

    out = out.squeeze().cpu().numpy()              # [pred_len]
    # é€†å½’ä¸€åŒ–
    out = scaler.inverse_transform(out.reshape(-1, 1)).flatten()  # [pred_len]
    preds.append(out)

# ==========================
# å¡«å…¥æäº¤æ¨¡æ¿
# ==========================
df_submit = df5.copy()

for i, pred in enumerate(preds):
    # å‡è®¾ df_submit æœ‰è¶³å¤Ÿè¡Œï¼Œè¿™é‡ŒæŒ‰æ¯æ®µé¢„æµ‹çš„é¡ºåºå¡«å…¥å‰ 3 è¡Œ
    for j, val in enumerate(pred):
        col_name = f"Prediction{j+1}"
        df_submit.loc[i, col_name] = val

# ==========================
# ä¿å­˜ CSV
# ==========================
df_submit.to_csv("æäº¤ç»“æœ_å¤èµ›_flow.csv", index=False)
print(df_submit.head())
torch.save(model.state_dict(), "last_dlinear_inflow.pth")
print("âœ… æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜ä¸º last_dlinear_inflow.pth")
